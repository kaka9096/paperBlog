📚 ```markdown
## 논문제목: MMInference: 장문 컨텍스트 VLMs의 사전 채우기 가속화를 위한 모달리티-인식 퍼뮤테이션 희소 주의

[http://arxiv.org/abs/2504.16083v1](http://arxiv.org/abs/2504.16083v1)

### 요약

비전-언어 모델(VLMs)의 장문 컨텍스트 능력 통합은 이전에는 상상하기 어려웠던 잠재력을 열어줍니다. 하지만 사전 채우기 단계에서 발생하는 이차적 주의 복잡도는 실질적인 활용에 큰 걸림돌이 됩니다. 이러한 한계를 극복하기 위해, 본 논문에서는 MMInference (Multimodality Million tokens Inference)라는 동적 희소 주의 방법을 소개합니다. MMInference는 장문 멀티모달 입력에 대한 사전 채우기 단계를 가속화하는 데 목적을 두고 있습니다.

**핵심 발견 사항:**

*   **Grid 패턴:** 비디오 입력의 시간적 및 공간적 지역성을 분석한 결과, 비디오 입력은 고유한 희소 패턴인 "Grid 패턴"을 나타내는 것으로 확인되었습니다.
*   **모달리티별 희소 분포:** VLMs는 서로 다른 모달리티에 따라 뚜렷하게 다른 희소 분포를 보입니다.
*   **퍼뮤테이션 기반 접근:** Grid 패턴을 활용하고 모달리티 경계 문제를 해결하기 위해 퍼뮤테이션 기반 방법을 도입했습니다.
*   **오프라인 검색:** 각 헤드에 대해 최적의 희소 패턴을 오프라인에서 검색하여 입력에 따라 동적으로 희소 분포를 구성합니다.
*   **GPU 최적화:** 효율적인 희소 계산을 위해 최적화된 GPU 커널을 제공합니다.

**주요 결과:**

*   MMInference는 기존 VLM 파이프라인에 원활하게 통합되며 모델 수정이나 미세 조정 없이 사용할 수 있습니다.
*   Video QA, Captioning, VisionNIAH, Mixed-Modality NIAH 등 다양한 멀티모달 벤치마크에서 최신 장문 VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL)와 함께 실험한 결과, 1M 토큰에서 사전 채우기 단계를 최대 8.3x 가속화하면서 정확도를 유지했습니다.

**코드 공개:** [https://aka.ms/MMInference](https://aka.ms/MMInference)

---

**참고:**

*   위 내용은 논문의 요약을 바탕으로 한국어로 번역된 것입니다.
*   MD 파일 형식으로 정리하여 가독성을 높였습니다.
*   논문 링크를 포함하여 원본 자료에 쉽게 접근할 수 있도록 했습니다.
*   핵심 발견 사항과 주요 결과를 강조하여 내용의 이해를 돕도록 했습니다.