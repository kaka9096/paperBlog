📚 # 논문제목: R1-Reward: Stable Reinforcement Learning을 통한 다중모달 보상 모델 훈련

[http://arxiv.org/abs/2505.02835v1](http://arxiv.org/abs/2505.02835v1)

## 요약

다중모달 대규모 언어 모델(MLLM)의 성능 향상에 중요한 역할을 하는 다중모달 보상 모델(MRM) 연구가 활발히 진행되고 있습니다. 최근 연구들은 MRM의 모델 구조 및 학습 데이터 개선에 초점을 맞추었지만, 보상 모델링에 장기 추론 능력의 활용과 이러한 능력을 활성화하는 방법에 대한 탐구는 부족했습니다.

본 논문에서는 강화 학습(RL)을 사용하여 보상 모델링을 개선하는 방안을 모색합니다. 특히, 보상 모델링 문제를 규칙 기반 RL 작업으로 재구성하여 접근했습니다. 하지만 기존 강화 학습 알고리즘(Reinforce++ 등)을 직접 적용하면 보상 모델링 과정에서 훈련 불안정성이나 심지어 붕괴 현상이 발생하는 것을 관찰했습니다.

이러한 문제를 해결하기 위해, 기존 RL 방법의 훈련 손실, 혜택 추정 전략, 보상 설계 등을 개선하는 StableReinforce 알고리즘을 제안합니다. 이러한 개선을 통해 보다 안정적인 훈련 동역학을 확보하고 우수한 성능을 달성할 수 있었습니다.

다양한 데이터셋에서 200K개의 선호 데이터셋을 수집하여 MRM 훈련을 용이하게 했습니다. StableReinforce 알고리즘을 사용하여 훈련된 보상 모델 R1-Reward는 이 데이터셋을 통해 훈련되어 다중모달 보상 모델링 벤치마크에서 상당한 성능 향상을 보였습니다. 기존 최첨단 모델(SOTA) 대비 VL Reward-Bench에서는 8.4%의 성능 향상, Multimodal Reward Bench에서는 14.3%의 성능 향상을 달성했습니다. 또한, 추론 컴퓨팅 자원이 증가함에 따라 R1-Reward의 성능은 더욱 향상되는 것을 확인하여 RL 알고리즘이 MRM 최적화에 미치는 잠재력을 강조했습니다.

## 주요 내용

*   **다중모달 보상 모델(MRM)의 중요성:** MLLM 성능 향상을 위한 핵심 요소
*   **RL을 통한 보상 모델링:** 규칙 기반 RL 작업으로의 재구성
*   **StableReinforce 알고리즘:** 기존 RL 알고리즘의 문제점 해결을 위한 개선된 방법
*   **200K 선호 데이터셋:** MRM 훈련을 위한 데이터셋 구축
*   **R1-Reward 모델:** StableReinforce 알고리즘 기반으로 훈련된 보상 모델
*   **벤치마크 성능 향상:** VL Reward-Bench (8.4%), Multimodal Reward Bench (14.3%)
*   **추론 컴퓨팅 자원 증가 시 성능 향상:** RL 알고리즘의 잠재력 강조

## 참고 자료

*   [http://arxiv.org/abs/2505.02835v1](http://arxiv.org/abs/2505.02835v1)