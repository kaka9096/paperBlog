📚 ## 논문제목: 픽셀 움직임을 위한 보편적 표현: 로봇 제어를 위한 LangToMo 프레임워크

[http://arxiv.org/abs/2505.07817v1](http://arxiv.org/abs/2505.07817v1)

**요약:**

본 논문에서는 픽셀 움직임 예측을 중간 표현으로 활용하는 비전-언어-행동 프레임워크인 LangToMo를 제시합니다.  LangToMo는 이중 시스템 아키텍처로 구성되어 있으며, 고수준의 시스템 2는 이미지 확산 모델로, 단일 프레임을 기반으로 텍스트 조건화된 픽셀 움직임 시퀀스를 생성하여 로봇 제어를 안내합니다. 픽셀 움직임은 보편적이고 해석 가능하며 움직임 중심적인 표현으로서, 비디오에서 자가 지도적으로 추출될 수 있으며, 이를 통해 웹 규모의 비디오-캡션 데이터에서 확산 모델 훈련이 가능합니다. 생성된 픽셀 움직임을 학습된 보편적 표현으로 취급하는 저수준의 시스템 1 모듈은 이러한 움직임을 로봇 행동으로 변환하는 움직임-행동 매핑 함수를 통해 수행합니다. 이 함수는 수동으로 제작하거나 최소한의 감독으로 학습할 수 있습니다. 시스템 2는 고수준 정책으로 희소 시간 간격으로 작동하는 반면, 시스템 1은 밀집 시간 간격으로 저수준 정책으로 작동합니다. 이러한 계층적 분리 아키텍처는 언어, 움직임 및 행동 간의 유연하고 확장 가능하며 일반화된 로봇 제어를 가능하게 합니다.  자세한 시각화를 보려면 다음 링크를 방문하십시오: [https://kahnchana.github.io/LangToMo](https://kahnchana.github.io/LangToMo)

**핵심 내용:**

*   **LangToMo 프레임워크:** 픽셀 움직임 예측을 핵심 중간 표현으로 사용하는 비전-언어-행동 프레임워크.
*   **이중 시스템 아키텍처:**
    *   **시스템 2 (고수준):** 이미지 확산 모델을 사용하여 텍스트 조건화된 픽셀 움직임 시퀀스 생성.
    *   **시스템 1 (저수준):** 생성된 픽셀 움직임을 로봇 행동으로 변환하는 움직임-행동 매핑 함수 사용 (수동 또는 최소 감독 학습).
*   **픽셀 움직임의 보편성:** 비디오에서 자가 지도적으로 추출 가능하며, 웹 규모의 데이터로 훈련에 적합.
*   **계층적 분리 아키텍처:** 유연하고 확장 가능하며 일반화된 로봇 제어 가능.
*   **언어, 움직임, 행동의 연결:** 언어, 움직임, 행동 간의 연결을 통해 로봇 제어의 효율성을 높임.

**주요 기술:**

*   이미지 확산 모델 (Image Diffusion Model)
*   자가 지도 학습 (Self-Supervised Learning)
*   움직임-행동 매핑 (Motion-to-Action Mapping)
*   언어-방식 연관 (Language-to-Motion)

**참고:** 이 번역은 원본 논문의 요약을 기반으로 작성되었으며, 전문 용어는 최대한 이해하기 쉽게 설명했습니다.  더 자세한 내용은 원본 논문을 참고하시기 바랍니다.