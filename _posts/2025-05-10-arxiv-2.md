📚 ## 논문제목: 3D Scene Generation: A Survey

[http://arxiv.org/abs/2505.05474v1](http://arxiv.org/abs/2505.05474v1)

### 요약

**3D Scene Generation 개요**

3D Scene Generation은 몰입형 미디어, 로봇 공학, 자율 주행, 상체 인공지능과 같은 응용 분야에 활용될 수 있는 공간적으로 구조화되고 의미 있는, 그리고 사진처럼 현실적인 환경을 합성하는 것을 목표로 합니다. 초기 방법들은 프로시저럴 규칙에 기반하여 확장성을 제공했지만 다양성은 제한되었습니다. 최근에는 GAN(Generative Adversarial Networks), 확산 모델(Diffusion Models)과 같은 딥 생성 모델, NeRF(Neural Radiance Fields), 3D Gaussians와 같은 3D 표현 방식의 발전으로 인해 실제 세계의 장면 분포를 학습하여 충실도, 다양성, 뷰 일관성을 향상시켰습니다. 특히 확산 모델은 3D 장면 합성 및 사진/영상 합성 문제를 연결하여 생성 과정을 재정의함으로써 사진/영상 합성의 가능성을 열었습니다.

**주요 내용**

이 논문은 최신 기술 동향을 체계적으로 개관하고, 다음 네 가지 패러다임으로 분류하여 분석합니다.

1.  **프로시저럴 생성 (Procedural Generation):**  규칙 기반으로 장면을 생성하는 방법.
2.  **신경망 기반 3D 생성 (Neural 3D-based Generation):**  딥러닝 모델을 사용하여 3D 장면을 생성하는 방법.
3.  **이미지 기반 생성 (Image-based Generation):**  2D 이미지 정보를 활용하여 3D 장면을 생성하는 방법.
4.  **영상 기반 생성 (Video-based Generation):**  영상 정보를 활용하여 3D 장면을 생성하는 방법.

논문은 각 방법론의 기술적 기반, 장단점, 대표적인 결과, 그리고 일반적으로 사용되는 데이터셋, 평가 프로토콜, 그리고 다운스트림 응용 분야를 분석합니다. 또한, 생성 능력, 3D 표현, 데이터 및 어노테이션, 평가와 관련된 주요 과제를 논의하고, 고품질, 물리적 지식 기반, 인터랙티브 생성, 그리고 통합적인 인지-생성 모델과 같은 유망한 방향을 제시합니다.

**추가 정보**

최신 개발 동향을 추적하기 위한 프로젝트 페이지를 GitHub에서 확인할 수 있습니다: [https://github.com/hzxie/Awesome-3D-Scene-Generation](https://github.com/hzxie/Awesome-3D-Scene-Generation)